<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Shutil on warrick.io</title>
    <link>//warrick.io/tags/shutil/</link>
    <description>Recent content in Shutil on warrick.io</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 15 Apr 2013 18:52:46 +0000</lastBuildDate>
    
	<atom:link href="//warrick.io/tags/shutil/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>working with large web resources in python</title>
      <link>//warrick.io/posts/working-with-large-web-resources-in-python/</link>
      <pubDate>Mon, 15 Apr 2013 18:52:46 +0000</pubDate>
      
      <guid>//warrick.io/posts/working-with-large-web-resources-in-python/</guid>
      <description>Saving the file to disk.1
import urllib2 import shutil req = urllib2.urlopen(url) with open(filename, &amp;#39;wb&amp;#39;) as f: shutil.copyfileobj(req, f) Reading GZIP compressed CSV files:2
import csv import gzip with gzip.open(filename) as f: reader = csv.reader(f, quoting=csv.QUOTE_NONE) header = csv.next() for row in reader: entry = dict(zip(header, row)) # ... http://stackoverflow.com/questions/9252812/using-csvreader-against-a-gzipped-file-in-python [return] http://stackoverflow.com/questions/1517616/stream-large-binary-files-with-urllib2-to-file [return]   </description>
    </item>
    
  </channel>
</rss>